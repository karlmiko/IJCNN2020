{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#CNN_GTzan-2D-75w-LoadAndTest-110250\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from numpy import *\n",
    "from keras import regularizers\n",
    "import os, sys\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, Dense, MaxPool1D, Flatten\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras import optimizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "import keras.initializers as init\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto( )\n",
    "config.gpu_options.allow_growth = True\n",
    "sess   = tf.Session(config=config)\n",
    "import keras.backend.tensorflow_backend as tf_bkend\n",
    "tf_bkend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controling Hyperparameters\n",
    "nb_classes = 10\n",
    "frame_size = 110250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indicate folds\n",
    "train_fold = [2, 3]\n",
    "test_fold = 1\n",
    "\n",
    "#Indicate mf or stft\n",
    "mf_or_stft = \"stft\"\n",
    "#If stft, we need a \"_stft\" at the end, otherwise empty \"\"...\n",
    "none_or_stft = \"_stft\"\n",
    "#Attacked: \"test_\" or \"BIM_\" or \"FGSM_\"\n",
    "attacked = \"BIM_\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = np.load(str(\"folds_\"+mf_or_stft+\"/2_GTzan_Xs_\"+attacked+\"fold\"\n",
    "                      +str(test_fold)+\"_110250_75_frozen\"+none_or_stft+\".npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_valid = np.load( str(\"folds_\"+mf_or_stft+\"/2_GTzan_Ys_test_fold\"\n",
    "                       +str(test_fold)+\"_110250_75_frozen\"+none_or_stft+\".npy\"))\n",
    "t_valid = np.load( str(\"folds_\"+mf_or_stft+\"/2_GTzan_ts_test_fold\"\n",
    "                       +str(test_fold)+\"_110250_75_frozen\"+none_or_stft+\".npy\"))\n",
    "s_valid = np.load( str(\"folds_\"+mf_or_stft+\"/2_GTzan_ss_test_fold\"\n",
    "                       +str(test_fold)+\"_110250_75_frozen\"+none_or_stft+\".npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7138, 257, 431), (7138, 10), (7138,), (7138,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape, Y_valid.shape, t_valid.shape, s_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7138, 257, 431, 1) (7138, 10)\n"
     ]
    }
   ],
   "source": [
    "f = X_valid.shape[1]\n",
    "g = X_valid.shape[2]\n",
    "\n",
    "# Adapt 1D data to 2D CNN\n",
    "X_valid = np.squeeze( X_valid )\n",
    "X_valid = np.expand_dims( X_valid, axis = 3 )\n",
    "\n",
    "print(X_valid.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generator_GTzannet2D_1a():\n",
    "    \n",
    "    from keras.layers      import Input, Dense, Conv2D, AveragePooling1D, LeakyReLU, MaxPool2D, Flatten\n",
    "    from keras.layers.core import Dropout\n",
    "    from keras.models      import Model\n",
    "    from keras             import initializers, optimizers, regularizers\n",
    "    from keras.callbacks   import ModelCheckpoint\n",
    "    from keras.utils       import multi_gpu_model\n",
    "    \n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "        \n",
    "    import keras.initializers as init\n",
    "    \n",
    "    from kapre.utils          import Normalization2D\n",
    "    from kapre.augmentation   import AdditiveNoise    \n",
    "    \n",
    "    sr = 22050\n",
    "    \n",
    "    inp   = Input(shape = (f, g, 1)) \n",
    "    #----------------------\n",
    "    conv1  = Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu')(inp)\n",
    "    norm1  = BatchNormalization()(conv1)\n",
    "    #----------------------\n",
    "    conv2  = Conv2D(filters = 32, kernel_size = (3, 3) )(norm1)\n",
    "    act2   = LeakyReLU(alpha = 0.2)(conv2)\n",
    "    pool2  = MaxPool2D(pool_size = 2, strides = 2)(act2)\n",
    "    drop2  = Dropout(0.05)(pool2)\n",
    "    #----------------------\n",
    "    conv3  = Conv2D(filters = 64, kernel_size = (3, 3) )(drop2)\n",
    "    act3   = LeakyReLU(alpha = 0.2)(conv3)\n",
    "    #----------------------\n",
    "    conv4  = Conv2D(filters = 64, kernel_size = (3, 3) )(act3)\n",
    "    act4   = LeakyReLU(alpha = 0.2)(conv4)\n",
    "    pool4  = MaxPool2D(pool_size = 4, strides = 2)(act4)\n",
    "    #----------------------\n",
    "    flat   = Flatten()(pool4)\n",
    "    #----------------------      \n",
    "    dense3 = Dense(512, activation='relu', kernel_initializer = initializers.glorot_uniform(seed = 0))(flat)\n",
    "    #dense3 = Dense(1024, activation='relu', kernel_initializer = initializers.glorot_uniform(seed = 0))(flat)\n",
    "    drop3  = Dropout(0.80)(dense3)    \n",
    "    #----------------------\n",
    "    dense4 = Dense(nb_classes, activation='softmax')(drop3)\n",
    "    #----------------------\n",
    "\n",
    "    model  = Model(inp, dense4)\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = optimizers.Adadelta(lr = 1.0, rho = 0.95, epsilon = 1e-08, decay = 0.0),\n",
    "                  metrics = ['accuracy'] )\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/akoerich/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/akoerich/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 257, 431, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 255, 429, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 255, 429, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 253, 427, 32)      9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 253, 427, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 126, 213, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 126, 213, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 124, 211, 64)      18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 124, 211, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 122, 209, 64)      36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 122, 209, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 60, 103, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 395520)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               202506752 \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 202,577,002\n",
      "Trainable params: 202,576,938\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Re-generating the model \n",
    "model = model_generator_GTzannet2D_1a()\n",
    "path = str( \"weights/weights_3_GTzan_3f_fold\"+str(train_fold[0])\n",
    "           +\"-\"+str(train_fold[1])+\"_20p_110250_75_frozen\"+none_or_stft+\".best.hdf5\" )\n",
    "model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights/weights_3_GTzan_3f_fold2-3_20p_110250_75_frozen_stft.best.hdf5\n",
      "Testing on test set of fold :1\n",
      "WARNING:tensorflow:From /home/akoerich/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      " 160/7138 [..............................] - ETA: 42:17"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "print(path)\n",
    "print(\"Testing on test set of fold :\" + str(test_fold)) \n",
    "print(model.evaluate(X_valid, Y_valid, verbose = 1))\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_valid.min(), t_valid.max(), s_valid.min(), s_valid.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t_valid), len(s_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zips t_valid, s_valid with value of their original index \n",
    "\n",
    "indexes = list(range(0,len(t_valid)))\n",
    "sorted_zip = sorted(zip(t_valid, s_valid, indexes))\n",
    "sorted_t_valid, sorted_s_valid, sorted_indexes = zip(*sorted_zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining similar segments\n",
    "\n",
    "big_matrix = [] # Stores list of windows for each track, with the track label\n",
    "\n",
    "print(\"Testing on test set of fold number: \" + str(test_fold))\n",
    "print_batch = 20       # Will print a message every 20 loops\n",
    "\n",
    "h = int(s_valid.max()) # all frames available\n",
    "beg = t_valid.min()    # smallest id value out of the track values\n",
    "end = t_valid.max()    # highest id value out of the track values\n",
    "similar_items_res_sum = list()\n",
    "y_target = None\n",
    "\n",
    "for i in range(0, len(sorted_t_valid)):\n",
    "    \n",
    "    if (beg != sorted_t_valid[i]):\n",
    "        \n",
    "        if (beg%print_batch == 0):\n",
    "            print(\"Done with:\", beg, \"out of\", end)\n",
    "        \n",
    "        beg = sorted_t_valid[i]\n",
    "        big_matrix.append([similar_items_res_sum, y_target])\n",
    "        similar_items_res_sum = list()\n",
    "    \n",
    "    j = sorted_indexes[i] # Index of the original unsorted list\n",
    "    \n",
    "    y_proba   = model.predict(X_valid[j:j+1])\n",
    "    #y_predict = y_proba.argmax()\n",
    "    y_target  = Y_valid[j:j+1].argmax(axis=-1)\n",
    "    \n",
    "    similar_items_res_sum.append(y_proba)\n",
    "    \n",
    "big_matrix.append([similar_items_res_sum, y_target]) #Append last list\n",
    "\n",
    "len_big_matrix = len(big_matrix)\n",
    "\n",
    "print(\"Done testing - Total samples:\", len_big_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Insert the number of splits (Example: 5 splits for 21 frames -> 1, 6, 12, 18, 21)\n",
    "\n",
    "splits = [1, 6, 12, 18, 21]\n",
    "        \n",
    "percentage_vot = []\n",
    "percentage_sum  = []\n",
    "\n",
    "for i in range (len(splits)):\n",
    "    percentage_vot.append(0)\n",
    "    percentage_sum.append(0)\n",
    "i = 0\n",
    "    \n",
    "for obj_similiar_items_res_sum in big_matrix:\n",
    "\n",
    "    similar_items_res_sum = obj_similiar_items_res_sum[0]\n",
    "    y_target = obj_similiar_items_res_sum[1]\n",
    "    \n",
    "    beg = 0\n",
    "    index = 0\n",
    "    lens = len(similar_items_res_sum) #To access the list of similar objects\n",
    "    candidates_vot = np.zeros(nb_classes)\n",
    "    candidates_sum = np.ones(nb_classes)\n",
    "    \n",
    "    for split in splits:\n",
    "        \n",
    "        current_split = split\n",
    "        end = min(lens, split)\n",
    "            \n",
    "        for n in range(beg, end):\n",
    "            candidates_vot[similar_items_res_sum[n].argmax()] += 1 #voting\n",
    "            candidates_sum = candidates_sum + similar_items_res_sum[n] #sum\n",
    "            \n",
    "        if (np.argmax(candidates_vot) == y_target):\n",
    "            percentage_vot[index] += 1\n",
    "        if (np.argmax(candidates_sum) == y_target):\n",
    "            percentage_sum[index] += 1\n",
    "            \n",
    "        candidates_sum = candidates_sum.copy()\n",
    "        candidates_vot = candidates_vot.copy()\n",
    "            \n",
    "        beg = current_split\n",
    "        index += 1\n",
    "        \n",
    "    i += 1\n",
    "    if (i%print_batch == 0):\n",
    "        print(\"Done with:\", i, \"out of\", len_big_matrix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### VOTING RULE\n",
    "i = 0\n",
    "for split in percentage_vot:\n",
    "    samples = len(big_matrix)\n",
    "    print( 'Acc:', ((split/samples)*100), 'Number of tracks tested:', samples, 'Number of frames combined:', splits[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUM RULE\n",
    "i = 0\n",
    "for split in percentage_sum:\n",
    "    samples = len(big_matrix)\n",
    "    print( 'Acc:', ((split/samples)*100), 'Number of tracks tested:', samples, 'Number of frames combined:', splits[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
