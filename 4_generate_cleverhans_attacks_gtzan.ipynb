{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack image files and test prediction after attacking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from CNN_GTzan-2D-75w-LoadAndTest-110250\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from numpy import *\n",
    "import soundfile as sf\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import backend\n",
    "import os, sys\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, Dense, MaxPool1D, Flatten\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras import optimizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "import keras.initializers as init\n",
    "\n",
    "from cleverhans.attacks import FastGradientMethod\n",
    "from cleverhans.attacks import BasicIterativeMethod\n",
    "#from cleverhans.attacks import DeepFool\n",
    "#from cleverhans.attacks import CarliniWagnerL2\n",
    "from cleverhans.utils_keras import KerasModelWrapper\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto( )\n",
    "config.gpu_options.allow_growth = True\n",
    "sess   = tf.Session(config=config)\n",
    "import keras.backend.tensorflow_backend as tf_bkend\n",
    "tf_bkend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controling Hyperparameters\n",
    "nb_classes = 10\n",
    "frame_size = 110250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indicate folds\n",
    "train_fold = [2, 3]\n",
    "test_fold = [1]\n",
    "\n",
    "o_fold = str(test_fold[0])\n",
    "f_s_folds = str(train_fold[0])+'-'+str(train_fold[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = np.load(str(\"folds_mf/2_GTzan_Xs_test_fold\"+o_fold+\"_110250_75_frozen.npy\"))\n",
    "#X_valid = np.load(str(\"folds_stft/2_GTzan_Xs_test_fold\"+o_fold+\"_110250_75_frozen_stft.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_valid = np.load( \"folds_mf/2_GTzan_Ys_test_fold\"+o_fold+\"_110250_75_frozen.npy\" )\n",
    "t_valid = np.load( \"folds_mf/2_GTzan_ts_test_fold\"+o_fold+\"_110250_75_frozen.npy\" )\n",
    "s_valid = np.load( \"folds_mf/2_GTzan_ss_test_fold\"+o_fold+\"_110250_75_frozen.npy\" )\n",
    "#Y_valid = np.load( \"folds_stft/2_GTzan_Ys_test_fold\"+o_fold+\"_110250_75_frozen_stft.npy\" )\n",
    "#t_valid = np.load( \"folds_stft/2_GTzan_ts_test_fold\"+o_fold+\"_110250_75_frozen_stft.npy\" )\n",
    "#s_valid = np.load( \"folds_stft/2_GTzan_ss_test_fold\"+o_fold+\"_110250_75_frozen_stft.npy\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7138, 64, 431, 1) (7138, 10)\n"
     ]
    }
   ],
   "source": [
    "#Adapt 1D data to 2D CNN\n",
    "\n",
    "X_valid = np.squeeze(X_valid)\n",
    "X_valid = np.expand_dims(X_valid, axis = 3)\n",
    "\n",
    "print(X_valid.shape, Y_valid.shape)\n",
    "\n",
    "len_X_valid = X_valid.shape[0]\n",
    "f = X_valid.shape[1]\n",
    "g = X_valid.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generator_GTzannet2D_1a():\n",
    "    \n",
    "    from keras.layers      import Input, Dense, Conv2D, AveragePooling1D, LeakyReLU, MaxPool2D, Flatten\n",
    "    from keras.layers.core import Dropout\n",
    "    from keras.models      import Model\n",
    "    from keras             import initializers, optimizers, regularizers\n",
    "    from keras.callbacks   import ModelCheckpoint\n",
    "    from keras.utils       import multi_gpu_model\n",
    "    \n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "        \n",
    "    import keras.initializers as init\n",
    "    \n",
    "    from kapre.utils          import Normalization2D\n",
    "    from kapre.augmentation   import AdditiveNoise    \n",
    "    \n",
    "    sr = 22050\n",
    "    \n",
    "    inp   = Input(shape = (f, g, 1))    \n",
    "    #----------------------\n",
    "    conv1  = Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu')(inp)\n",
    "    norm1  = BatchNormalization()(conv1)\n",
    "    #----------------------\n",
    "    conv2  = Conv2D(filters = 32, kernel_size = (3, 3) )(norm1)\n",
    "    act2   = LeakyReLU(alpha = 0.2)(conv2)\n",
    "    pool2  = MaxPool2D(pool_size = 2, strides = 2)(act2)\n",
    "    drop2  = Dropout(0.05)(pool2)\n",
    "    #----------------------\n",
    "    conv3  = Conv2D(filters = 64, kernel_size = (3, 3) )(drop2)\n",
    "    act3   = LeakyReLU(alpha = 0.2)(conv3)\n",
    "    #----------------------\n",
    "    conv4  = Conv2D(filters = 64, kernel_size = (3, 3) )(act3)\n",
    "    act4   = LeakyReLU(alpha = 0.2)(conv4)\n",
    "    pool4  = MaxPool2D(pool_size = 4, strides = 2)(act4)\n",
    "    #----------------------\n",
    "    flat   = Flatten()(pool4)\n",
    "    #----------------------    \n",
    "    #dense3 = Dense(512, activation='relu', kernel_initializer = initializers.glorot_uniform( seed = 0) )(flat)\n",
    "    dense3 = Dense(1024, activation='relu', kernel_initializer = initializers.glorot_uniform( seed = 0) )(flat)\n",
    "    drop3  = Dropout(0.80)(dense3)    \n",
    "    #----------------------\n",
    "    dense4 = Dense(nb_classes, activation='softmax')(drop3)\n",
    "    #----------------------\n",
    "\n",
    "    model  = Model(inp, dense4)\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = optimizers.Adadelta( lr = 1.0, rho = 0.95, epsilon = 1e-08, decay = 0.0),\n",
    "                  metrics = ['accuracy'] )\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 431, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 62, 429, 32)       320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 62, 429, 32)       128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 60, 427, 32)       9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 60, 427, 32)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 213, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 213, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 211, 64)       18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 28, 211, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 26, 209, 64)       36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 26, 209, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 103, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 79104)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              81003520  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 81,078,890\n",
      "Trainable params: 81,078,826\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Re-generating the model \n",
    "model = model_generator_GTzannet2D_1a()\n",
    "path = \"weights/weights_3_GTzan_3f_fold\"+f_s_folds+\"_20p_110250_75_frozen.best.hdf5\"\n",
    "#path = \"weights/weights_3_GTzan_3f_fold\"+f_s_folds+\"_20p_110250_75_frozen_stft.best.hdf5\"\n",
    "model.load_weights(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If adversarial samples already generated, skip section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Attack objects\n",
    "\n",
    "wrap = KerasModelWrapper(model)\n",
    "#attack = FastGradientMethod(wrap, sess=sess)\n",
    "attack = BasicIterativeMethod(wrap, sess=sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7138, 64, 431)\n"
     ]
    }
   ],
   "source": [
    "# Create a big matrix to accomodate test files\n",
    "\n",
    "adversarial_matrix = np.ones((len_X_valid, f, g), dtype=float)\n",
    "print(adversarial_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/akoerich/.local/lib/python3.5/site-packages/cleverhans/attacks.py:390: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/akoerich/.local/lib/python3.5/site-packages/cleverhans/attacks_tf.py:62: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/akoerich/.local/lib/python3.5/site-packages/cleverhans/utils_tf.py:37: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Frames attacked: 0\n",
      "Frames attacked: 500\n",
      "Frames attacked: 1000\n",
      "Frames attacked: 1500\n",
      "Frames attacked: 2000\n",
      "Frames attacked: 2500\n",
      "Frames attacked: 3000\n",
      "Frames attacked: 3500\n",
      "Frames attacked: 4000\n",
      "Frames attacked: 4500\n",
      "Frames attacked: 5000\n",
      "Frames attacked: 5500\n",
      "Frames attacked: 6000\n",
      "Frames attacked: 6500\n",
      "Frames attacked: 7000\n",
      "Adversarial matrix filled\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len_X_valid):\n",
    "    # Generates the adversarial example\n",
    "    adversarial_example = attack.generate_np(X_valid[i:i+1], eps=0.3, eps_iter=0.1, nb_iter=10)\n",
    "    adversarial_matrix[i,:] = np.squeeze(adversarial_example)\n",
    "    if (i%500 == 0.000):\n",
    "        print(\"Frames attacked: \" +str(i))\n",
    "print(\"Adversarial matrix filled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save data files (input arrays) to disk (.npy files)\n",
    "\n",
    "np.save( \"folds_mf/2_GTzan_Xs_BIM_fold\"+o_fold+\"_110250_75_frozen.npy\", adversarial_matrix)\n",
    "#np.save( \"folds_stft/2_GTzan_Xs_BIM_fold\"+o_fold+\"_110250_75_frozen_stft.npy\", adversarial_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of part to skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load matrix with adversarial samples of test fold\n",
    "\n",
    "adversarial_matrix = np.load(\"folds_mf/2_GTzan_Xs_BIM_fold\"+o_fold+\"_110250_75_frozen.npy\")\n",
    "#adversarial_matrix = np.load(\"folds_stft/2_GTzan_Xs_BIM_fold\"+o_fold+\"_110250_75_frozen_stft.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zips t_valid, s_valid with value of their original index \n",
    "\n",
    "indexes = list(range(0,len(t_valid)))\n",
    "sorted_zip = sorted(zip(t_valid, s_valid, indexes))\n",
    "sorted_t_valid, sorted_s_valid, sorted_indexes = zip(*sorted_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on test set of fold number: [1]\n",
      "Done with: 665 out of 1000\n",
      "Done with: 670 out of 1000\n",
      "Done with: 675 out of 1000\n",
      "Done with: 680 out of 1000\n",
      "Done with: 685 out of 1000\n",
      "Done with: 690 out of 1000\n",
      "Done with: 695 out of 1000\n",
      "Done with: 700 out of 1000\n",
      "Done with: 705 out of 1000\n",
      "Done with: 710 out of 1000\n",
      "Done with: 715 out of 1000\n",
      "Done with: 720 out of 1000\n",
      "Done with: 725 out of 1000\n",
      "Done with: 730 out of 1000\n",
      "Done with: 735 out of 1000\n",
      "Done with: 740 out of 1000\n",
      "Done with: 745 out of 1000\n",
      "Done with: 750 out of 1000\n",
      "Done with: 755 out of 1000\n",
      "Done with: 760 out of 1000\n",
      "Done with: 765 out of 1000\n",
      "Done with: 770 out of 1000\n",
      "Done with: 775 out of 1000\n",
      "Done with: 780 out of 1000\n",
      "Done with: 785 out of 1000\n",
      "Done with: 790 out of 1000\n",
      "Done with: 795 out of 1000\n",
      "Done with: 800 out of 1000\n",
      "Done with: 805 out of 1000\n",
      "Done with: 810 out of 1000\n",
      "Done with: 815 out of 1000\n",
      "Done with: 820 out of 1000\n",
      "Done with: 825 out of 1000\n",
      "Done with: 830 out of 1000\n",
      "Done with: 835 out of 1000\n",
      "Done with: 840 out of 1000\n",
      "Done with: 845 out of 1000\n",
      "Done with: 850 out of 1000\n",
      "Done with: 855 out of 1000\n",
      "Done with: 860 out of 1000\n",
      "Done with: 865 out of 1000\n",
      "Done with: 870 out of 1000\n",
      "Done with: 875 out of 1000\n",
      "Done with: 880 out of 1000\n",
      "Done with: 885 out of 1000\n",
      "Done with: 890 out of 1000\n",
      "Done with: 895 out of 1000\n",
      "Done with: 900 out of 1000\n",
      "Done with: 905 out of 1000\n",
      "Done with: 910 out of 1000\n",
      "Done with: 915 out of 1000\n",
      "Done with: 920 out of 1000\n",
      "Done with: 925 out of 1000\n",
      "Done with: 930 out of 1000\n",
      "Done with: 935 out of 1000\n",
      "Done with: 940 out of 1000\n",
      "Done with: 945 out of 1000\n",
      "Done with: 950 out of 1000\n",
      "Done with: 955 out of 1000\n",
      "Done with: 960 out of 1000\n",
      "Done with: 965 out of 1000\n",
      "Done with: 970 out of 1000\n",
      "Done with: 975 out of 1000\n",
      "Done with: 980 out of 1000\n",
      "Done with: 985 out of 1000\n",
      "Done with: 990 out of 1000\n",
      "Done with: 995 out of 1000\n",
      "Done testing - Total samples: 340\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Combining similar segments\n",
    "\n",
    "big_matrix = [] # Stores list of windows for each track, with the track label\n",
    "\n",
    "print(\"Testing on test set of fold number: \" + str(test_fold))\n",
    "print_batch = 5       # Will print a message every 5 loops\n",
    "\n",
    "m = 0\n",
    "acc = 0\n",
    "\n",
    "h = int(s_valid.max()) # all frames available\n",
    "beg = t_valid.min()    # smallest id value out of the track values\n",
    "end = t_valid.max()    # highest id value out of the track values\n",
    "similar_items_res_sum = list()\n",
    "y_target = None\n",
    "\n",
    "for i in range(0, len(sorted_t_valid)):\n",
    "    \n",
    "    if (beg != sorted_t_valid[i]):\n",
    "        \n",
    "        if (beg%print_batch == 0):\n",
    "            print(\"Done with:\", beg, \"out of\", end)\n",
    "        \n",
    "        beg = sorted_t_valid[i]\n",
    "        big_matrix.append([similar_items_res_sum, y_target])\n",
    "        similar_items_res_sum = list()\n",
    "    \n",
    "    j = sorted_indexes[i] # Index of the original unsorted list\n",
    "    \n",
    "    # Loads the adversarial example\n",
    "    adversarial_example = adversarial_matrix[j:j+1]\n",
    "    \n",
    "    y_proba   = model.predict(np.expand_dims(adversarial_example, axis=3))\n",
    "    y_target  = Y_valid[j:j+1].argmax(axis=-1)\n",
    "    \n",
    "    m += 1\n",
    "    if (y_proba.argmax() == Y_valid[j:j+1].argmax(axis=-1)):\n",
    "        acc += 1\n",
    "    \n",
    "    similar_items_res_sum.append(y_proba)\n",
    "    \n",
    "big_matrix.append([similar_items_res_sum, y_target]) #Append last list\n",
    "\n",
    "len_big_matrix = len(big_matrix)\n",
    "\n",
    "print(\"Done testing - Total samples:\", len_big_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936 7138 0.1311291678341272\n"
     ]
    }
   ],
   "source": [
    "#Accuracy on the entire set\n",
    "print (acc, m, acc/m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with: 5 out of 340\n",
      "Done with: 10 out of 340\n",
      "Done with: 15 out of 340\n",
      "Done with: 20 out of 340\n",
      "Done with: 25 out of 340\n",
      "Done with: 30 out of 340\n",
      "Done with: 35 out of 340\n",
      "Done with: 40 out of 340\n",
      "Done with: 45 out of 340\n",
      "Done with: 50 out of 340\n",
      "Done with: 55 out of 340\n",
      "Done with: 60 out of 340\n",
      "Done with: 65 out of 340\n",
      "Done with: 70 out of 340\n",
      "Done with: 75 out of 340\n",
      "Done with: 80 out of 340\n",
      "Done with: 85 out of 340\n",
      "Done with: 90 out of 340\n",
      "Done with: 95 out of 340\n",
      "Done with: 100 out of 340\n",
      "Done with: 105 out of 340\n",
      "Done with: 110 out of 340\n",
      "Done with: 115 out of 340\n",
      "Done with: 120 out of 340\n",
      "Done with: 125 out of 340\n",
      "Done with: 130 out of 340\n",
      "Done with: 135 out of 340\n",
      "Done with: 140 out of 340\n",
      "Done with: 145 out of 340\n",
      "Done with: 150 out of 340\n",
      "Done with: 155 out of 340\n",
      "Done with: 160 out of 340\n",
      "Done with: 165 out of 340\n",
      "Done with: 170 out of 340\n",
      "Done with: 175 out of 340\n",
      "Done with: 180 out of 340\n",
      "Done with: 185 out of 340\n",
      "Done with: 190 out of 340\n",
      "Done with: 195 out of 340\n",
      "Done with: 200 out of 340\n",
      "Done with: 205 out of 340\n",
      "Done with: 210 out of 340\n",
      "Done with: 215 out of 340\n",
      "Done with: 220 out of 340\n",
      "Done with: 225 out of 340\n",
      "Done with: 230 out of 340\n",
      "Done with: 235 out of 340\n",
      "Done with: 240 out of 340\n",
      "Done with: 245 out of 340\n",
      "Done with: 250 out of 340\n",
      "Done with: 255 out of 340\n",
      "Done with: 260 out of 340\n",
      "Done with: 265 out of 340\n",
      "Done with: 270 out of 340\n",
      "Done with: 275 out of 340\n",
      "Done with: 280 out of 340\n",
      "Done with: 285 out of 340\n",
      "Done with: 290 out of 340\n",
      "Done with: 295 out of 340\n",
      "Done with: 300 out of 340\n",
      "Done with: 305 out of 340\n",
      "Done with: 310 out of 340\n",
      "Done with: 315 out of 340\n",
      "Done with: 320 out of 340\n",
      "Done with: 325 out of 340\n",
      "Done with: 330 out of 340\n",
      "Done with: 335 out of 340\n",
      "Done with: 340 out of 340\n"
     ]
    }
   ],
   "source": [
    "#Insert the number of splits (Example: 5 splits for 21 frames -> 1, 6, 12, 18, 21)\n",
    "\n",
    "splits = [1, 6, 12, 18, 21]\n",
    "        \n",
    "percentage_vot = []\n",
    "percentage_sum  = []\n",
    "\n",
    "for i in range (len(splits)):\n",
    "    percentage_vot.append(0)\n",
    "    percentage_sum.append(0)\n",
    "i = 0\n",
    "    \n",
    "for obj_similiar_items_res_sum in big_matrix:\n",
    "\n",
    "    similar_items_res_sum = obj_similiar_items_res_sum[0]\n",
    "    y_target = obj_similiar_items_res_sum[1]\n",
    "    \n",
    "    beg = 0\n",
    "    index = 0\n",
    "    lens = len(similar_items_res_sum) #To access the list of similar objects\n",
    "    candidates_vot = np.zeros(nb_classes)\n",
    "    candidates_sum = np.ones(nb_classes)\n",
    "    \n",
    "    for split in splits:\n",
    "        \n",
    "        current_split = split\n",
    "        end = min(lens, split)\n",
    "            \n",
    "        for n in range(beg, end):\n",
    "            candidates_vot[np.asarray(similar_items_res_sum[n]).argmax()] += 1 #voting\n",
    "            candidates_sum = candidates_sum + similar_items_res_sum[n] #sum\n",
    "            \n",
    "        if (np.argmax(candidates_vot) == y_target):\n",
    "            percentage_vot[index] += 1\n",
    "        if (np.argmax(candidates_sum) == y_target):\n",
    "            percentage_sum[index] += 1\n",
    "            \n",
    "        candidates_sum = candidates_sum.copy()\n",
    "        candidates_vot = candidates_vot.copy()\n",
    "            \n",
    "        beg = current_split\n",
    "        index += 1\n",
    "        \n",
    "    i += 1\n",
    "    if (i%print_batch == 0):\n",
    "        print(\"Done with:\", i, \"out of\", len_big_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 12.647058823529411 Number of tracks tested: 340 Number of frames combined: 1\n",
      "Acc: 10.0 Number of tracks tested: 340 Number of frames combined: 6\n",
      "Acc: 9.411764705882353 Number of tracks tested: 340 Number of frames combined: 12\n",
      "Acc: 10.294117647058822 Number of tracks tested: 340 Number of frames combined: 18\n",
      "Acc: 9.705882352941178 Number of tracks tested: 340 Number of frames combined: 21\n"
     ]
    }
   ],
   "source": [
    "###### VOTING RULE\n",
    "i = 0\n",
    "samples = len(big_matrix)\n",
    "for split in percentage_vot:\n",
    "    print( 'Acc:', ((split/samples)*100), 'Number of tracks tested:', samples, 'Number of frames combined:', splits[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 12.647058823529411 Number of tracks tested: 340 Number of frames combined: 1\n",
      "Acc: 10.882352941176471 Number of tracks tested: 340 Number of frames combined: 6\n",
      "Acc: 8.529411764705882 Number of tracks tested: 340 Number of frames combined: 12\n",
      "Acc: 10.294117647058822 Number of tracks tested: 340 Number of frames combined: 18\n",
      "Acc: 10.294117647058822 Number of tracks tested: 340 Number of frames combined: 21\n"
     ]
    }
   ],
   "source": [
    "###### SUM RULE\n",
    "i = 0\n",
    "samples = len(big_matrix)\n",
    "for split in percentage_sum:\n",
    "    print( 'Acc:', ((split/samples)*100), 'Number of tracks tested:', samples, 'Number of frames combined:', splits[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
